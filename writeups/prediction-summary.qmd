---
title: "Predictive modeling of claims status"
author: 'Ruizhe Jiang, Sam Su, Sumeng Xu, Jaxon Zhang'
date: "November 20, 2024"
---

### Abstract

In this project, we developed predictive models for binary and multiclass classification of webpage claims using transformer-based architectures. The input data consisted of raw HTML content, from which text was extracted, cleaned, and preprocessed. For binary classification, a BERT-based transformer model was fine-tuned to distinguish between two classes, achieving an accuracy of 82% on the test data. For multiclass classification, another fine-tuned BERT model was used to predict five claim categories, achieving an accuracy of 84.57%. Both models demonstrated the strength of transformer architectures in handling textual data for classification tasks.

### Preprocessing

The preprocessing pipeline involved extracting visible text content from raw HTML files using the BeautifulSoup library, removing irrelevant tags, scripts, and metadata. The cleaned text was then tokenized using the pretrained BERT tokenizer, with truncation and padding applied to ensure all inputs were of uniform length. This tokenization converted the cleaned text into token sequences, which were represented numerically as contextual embeddings generated by the BERT tokenizer. These embeddings served as the input to the transformer models for training and inference.

### Methods

#### Binary Classification

The binary classification model was built using a fine-tuned BERT-based transformer model. The architecture was specified with a single linear output layer consisting of two nodes (for the binary labels) and a softmax activation function to generate class probabilities. The model utilized the pretrained BERT weights to generate contextual embeddings for the tokenized input text. Hyperparameters were selected based on standard fine-tuning practices, with a learning rate of 2e-5, a batch size of 16, and a maximum sequence length of 512 tokens. The training was performed using the Adam optimizer with weight decay to prevent overfitting and cross-entropy loss as the objective function. Early stopping was implemented to avoid overfitting, and the model was evaluated on a validation set after each epoch to track performance. The training process ran for 5 epochs, and the model achieved an accuracy of 82% on the final validation data.

#### Multiclass Classification

The multiclass classification model used the same BERT-based transformer architecture but was adapted to handle five output classes. The model's final linear output layer consisted of five nodes, with a softmax activation function to output probabilities for each class. Hyperparameter selection mirrored the binary classification approach, with a learning rate of 2e-5, a batch size of 16, and a sequence length of 512. The training was again performed using the Adam optimizer with weight decay and cross-entropy loss as the loss function. The model was trained for 5 epochs, and early stopping was applied to prevent overfitting. Evaluation on the validation set ensured that the model generalized well to unseen data. The final model achieved an accuracy of 84.57% on the final validation dataset.

### Results

#### Binary Classification

The binary classification model achieves the following metrics on the validation dataset:

| Metric        | Value     |
|---------------|-----------|
| Accuracy      | 82.01%    |
| F1 Score      | 83.22%    |
| Precision     | 82.68%    |
| Recall        | 83.77%    |
| Loss          | 0.4132    |

#### Multiclass Classification

The multiclass classification model's performance is outlined below:

| Metric        | Value     |
|---------------|-----------|
| Accuracy      | 84.58%    |
| F1 Score      | 84.47%    |
| Precision     | 84.51%    |
| Recall        | 84.58%    |
| Loss          | 0.4570    |

The model achieved consistent precision and recall across the five classes, as reflected in its F1 score of 84.47%. These results validate the transformer modelâ€™s ability to handle multiclass classification tasks effectively.
