---
title: "Summary of exploratory tasks"
author: 'Ruizhe Jiang, Sam Su, Sumeng Xu, Jaxon Zhang'
date: today
---

### HTML scraping

Using logistic principal component regression improved accuracy. The test accuracy before adding the Header Information is 0.59. After adding the 6 header tags, the test accuracy is 0.77. Additionally, under Unigram model PCA, the AIC is 18. While preparing the data for developing the predictive model, the binary classification testing AUC was approximately 0.75, incorporating header information as well as several other details.

We also explored changes in performance using the given NLP FNN model, which included a connected layer with 25 neurons (units). Including header content in the model resulted in a small improvement in performance, primarily on the training data. The binary accuracy increased from 0.92 to 0.95, indicating a small improvement in the model's ability to classify the training data correctly, demonstrating better model fitting. The loss decreased throughout the five epochs, ending at 0.27 compared to 0.31, further indicating improved model fitting. On the other hand, the changes were less pronounced in the validation set. The validation binary accuracy increased slightly but stayed around 0.8, suggesting no significant improvement in the model's generalization to unseen data. However, the validation loss decreased slightly from 0.83 to 0.79, indicating a minor improvement in performance on the validation set. Overall, including header content seems to enhance the model’s performance on the training data, but its impact on predictive accuracy for unseen data is minimal.

### Bigrams

First we prepared the text data (claims-raw.RData) for analysis by tokenizing it into unigrams and bigrams, reducing the dimensionality using PCA, and then building two logistic regression models (one with only unigram features and one with both unigram and bigram features). Then we compared the models using AIC to decide whether adding bigrams improves the model performance.

Based on our analysis, adding bigrams doesn’t really help in figuring out the claims status of a page. First, a model was created using single words, or unigrams, and it had an AIC score of 18. Then, the text was split into pairs of words, bigrams, and another model was made that combined the unigrams with the bigrams. This combined model also had an AIC score of 18, the same as the unigram model. Since the scores are the same, it means the bigrams didn’t add anything useful to the predictions. Overall, the single words were enough to predict the claims status, and the bigrams didn’t make any difference.

### Neural net

The neural network models for both binary and multiclass classification tasks were built using a fine-tuned BERT-based transformer architecture, which leverages pretrained embeddings to extract contextual features from textual data. The architecture consisted of a BERT encoder followed by a classification head with a single fully connected layer. For binary classification, the output layer contained two neurons, while for multiclass classification, it contained five neurons, both with a softmax activation function for generating class probabilities.

The optimization was performed using the Adam optimizer with a learning rate of 2e-5 and weight decay for regularization. The cross-entropy loss function was used as the objective for both tasks to handle classification effectively. The models were trained for 5 epochs with early stopping to avoid overfitting, and validation was conducted after each epoch to monitor performance.

The binary classification model achieved a predictive accuracy of 82.01%, while the multiclass classification model achieved an accuracy of 84.58% on the validation set. These results demonstrate the neural network's ability to accurately predict both binary and multiclass labels for textual claim data.

### Primary Task Attempt

> *A logistic regression `bclass_logreg_pca.ipynb` with a slightly modified architecture was developed as well for binary classification. Similar preprocessing techniques were used. The final test accuracy was 80.74% using the metric of weighted F1 score.*

For the principle logistic regression with modified architecture, - The `parse_data()` function from `preprocessing.R` is used in addition to the customized function to capture the headers and paragraphs information from the raw HTML.

-   All punctuation words and stopped words were removed. Every letters were converted to lower case. Additionally, tokenization and lemmatization were conducted using `Spacy` package.

-   Different numbers of pronouns were counted and treated as additional features for training the model. Term Frequency-Inverse Term Frequency (tf-idf) vectorization was performed to weight the tokenized unigrams and bigrams. Logit-odds from unigrams after PCA were also treated as one of the numerical features.

For the principal logistic regression with a modified architecture, two logistic principal component regression models with L1 penalty were performed on unigrams and bigrams vectorized data, respectively. Grid search with 5-fold cross-validation was used to determine the optimal regularization parameter and exclude features with lower importance based on their contributions to the covariance matrix. The final principle features were combined with logit-odds from unigrams and numbers of pronouns to form a final `X_features`.
