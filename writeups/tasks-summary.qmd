---
title: "Summary of exploratory tasks"
author: 'Ruizhe Jiang, Sam Su, Sumeng Xu, Jaxon Zhang'
date: today
---

### HTML scraping

Using logistic principal component regression did not significantly improve accuracy. Under Unigram model PCA, the AIC is 18. While preparing the data for developing the predictive model, the binary classification testing AUC was approximately 0.75, incorporating header information as well as several other details. The improvement was minimal. We also explored changes in performance using the given NLP FNN model, which included a connected layer with 25 neurons (units). Including header content in the model resulted in a small improvement in performance, primarily on the training data. The binary accuracy increased from 0.92 to 0.95, indicating a small improvement in the model's ability to classify the training data correctly, demonstrating better model fitting. The loss decreased throughout the five epochs, ending at 0.27 compared to 0.31, further indicating improved model fitting. On the other hand, the changes were less pronounced in the validation set. The validation binary accuracy increased slightly but stayed around 0.8, suggesting no significant improvement in the model's generalization to unseen data. However, the validation loss decreased slightly from 0.83 to 0.79, indicating a minor improvement in performance on the validation set. Overall, including header content seems to enhance the model’s performance on the training data, but its impact on predictive accuracy for unseen data is minimal. 


### Bigrams

Do bigrams capture additional information relevant to the classification of interest? Answer the question, **briefly** describe what analysis you conducted to arrive at your answer, and provide quantitative evidence supporting your answer.

First we prepared the text data (claims-raw.RData) for analysis by tokenizing it into unigrams and bigrams, reducing the dimensionality using PCA, and then building two logistic regression models (one with only unigram features and one with both unigram and bigram features). Then we compared the models using AIC to decide whether adding bigrams improves the model performance.

Based on our analysis, adding bigrams doesn’t really help in figuring out the claims status of a page. First, a model was created using single words, or unigrams, and it had an AIC score of 18. Then, the text was split into pairs of words, bigrams, and another model was made that combined the unigrams with the bigrams. This combined model also had an AIC score of 18, the same as the unigram model. Since the scores are the same, it means the bigrams didn’t add anything useful to the predictions. Overall, the single words were enough to predict the claims status, and the bigrams didn’t make any difference.

### Neural net

Summarize the neural network model you trained by desribing:

-   architecture

-   optimization and loss

-   training epochs

-   predictive accuracy


### Primary Task Attempt 
> *A logistic regression `bclass_logreg_pca.ipynb` with a slightly modified architecture was developed as well for binary classification. Similar preprocessing techniques were used. The final test accuracy was 80.74% using the metric of weighted F1 score.*

For the principle logistic regression with modified architecture,
-   The `parse_data()` function from `preprocessing.R` is used in addition to the customized function to capture the headers and paragraphs information from the raw HTML.

-   All punctuation words and stopped words were removed. Every letters were converted to lower case. Additionally, tokenization and lemmatization were conducted using `Spacy` package. 

-   Different numbers of pronouns were counted and treated as additional features for training the model. Term Frequency-Inverse Term Frequency (tf-idf) vectorization was performed to weight the tokenized unigrams and bigrams. Logit-odds from unigrams after PCA were also treated as one of the numerical features.

For the principal logistic regression with a modified architecture, two logistic principal component regression models with L1 penalty were performed on unigrams and bigrams vectorized data, respectively. Grid search with 5-fold cross-validation was used to determine the optimal regularization parameter and exclude features with lower importance based on their contributions to the covariance matrix. The final principle features were combined with logit-odds from unigrams and numbers of pronouns to form a final `X_features`.