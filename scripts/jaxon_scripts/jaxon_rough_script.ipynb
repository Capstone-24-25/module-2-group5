{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean = pd.read_csv('../data/claims_clean.csv')\n",
    "# df_raw = pd.read_csv('../data/claims_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_clean.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTML Texts Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw['soup'] = df_raw['text_tmp'].apply(lambda x: BeautifulSoup(x, 'html.parser'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_headers(soup):\n",
    "#   headers = []\n",
    "#   for header_tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:\n",
    "#     for header in soup.find_all(header_tag):\n",
    "#       headers.append(header.get_text(strip=True))\n",
    "#   headers = '. '.join(headers)\n",
    "#   return headers\n",
    "\n",
    "# def extract_paragraphs(soup):\n",
    "#   contents = []\n",
    "#   for paragraph in soup.find_all('p'):\n",
    "#     contents.append(paragraph.get_text(strip=True))\n",
    "#   contents = '. '.join(contents)\n",
    "#   return contents\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_raw['headers'] = df_raw['soup'].apply(extract_headers)\n",
    "# df_raw['paragraphs'] = df_raw['soup'].apply(extract_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_temp = pd.concat((df_clean['text_clean'], df_raw[['headers', 'paragraphs']]), axis=1) # concat h, p, and clean texts\n",
    "# df_temp = df_temp.apply(lambda row: '\\n'.join(row.values.astype(str)), axis=1) # join them to a single cell\n",
    "# df_model = pd.concat((df_temp, df_raw[['mclass', 'bclass']]), axis = 1) # add y to the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model.rename(columns={0: 'contents'}, inplace=True)\n",
    "# df_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Texts Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model = pd.read_csv('../data/data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_text(text):\n",
    "#   text = text.lower() # lower all the texts\n",
    "\n",
    "#   # text = re.sub(r'http\\S+|www.\\S+', '', text) # remove all the links\n",
    "\n",
    "#   doc = nlp(text)\n",
    "#   cleaned_tokens = []\n",
    "\n",
    "#   for token in doc:\n",
    "#     if not token.is_punct and not token.is_stop: # remove punctuation and stop words\n",
    "#       cleaned_tokens.append(token.lemma_) # lemmatization\n",
    "\n",
    "#   return ' '.join(cleaned_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takes long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model['cleaned_text'] = df_model['contents'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model = pd.read_csv('../data/data_2_after_tokenization_lemmatization.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>contents</th>\n",
       "      <th>mclass</th>\n",
       "      <th>bclass</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>national obituary search click on the item you...</td>\n",
       "      <td>Possible Fatality</td>\n",
       "      <td>Relevant claim content</td>\n",
       "      <td>national obituary search click item like print...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the following official arrest record for jonat...</td>\n",
       "      <td>Potentially unlawful activity</td>\n",
       "      <td>Relevant claim content</td>\n",
       "      <td>follow official arrest record jonathan andrew ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>did someone you know get arrested in miami dad...</td>\n",
       "      <td>N/A: No relevant content.</td>\n",
       "      <td>N/A: No relevant content.</td>\n",
       "      <td>know arrest miami dade county fl find busted m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the information on this website is taken from ...</td>\n",
       "      <td>Potentially unlawful activity</td>\n",
       "      <td>Relevant claim content</td>\n",
       "      <td>information website take record available stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>name clayton thomas location memphis tennessee...</td>\n",
       "      <td>Potentially unlawful activity</td>\n",
       "      <td>Relevant claim content</td>\n",
       "      <td>clayton thomas location memphis tennessee age ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            contents  \\\n",
       "0  national obituary search click on the item you...   \n",
       "1  the following official arrest record for jonat...   \n",
       "2  did someone you know get arrested in miami dad...   \n",
       "3  the information on this website is taken from ...   \n",
       "4  name clayton thomas location memphis tennessee...   \n",
       "\n",
       "                          mclass                     bclass  \\\n",
       "0              Possible Fatality     Relevant claim content   \n",
       "1  Potentially unlawful activity     Relevant claim content   \n",
       "2      N/A: No relevant content.  N/A: No relevant content.   \n",
       "3  Potentially unlawful activity     Relevant claim content   \n",
       "4  Potentially unlawful activity     Relevant claim content   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  national obituary search click item like print...  \n",
       "1  follow official arrest record jonathan andrew ...  \n",
       "2  know arrest miami dade county fl find busted m...  \n",
       "3  information website take record available stat...  \n",
       "4  clayton thomas location memphis tennessee age ...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_model.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_doc_vector(text):\n",
    "#   doc = nlp(text)\n",
    "#   return doc.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take long time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_model['doc_vector'] = df_model['cleaned_text'].apply(get_doc_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = TfidfVectorizer(max_features=5000)\n",
    "# tfidf_matrix = tfidf.fit_transform(df_model['cleaned_text'])\n",
    "# df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "#                         columns=tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_doc_vectors = pd.DataFrame(df_model['doc_vector'].tolist())\n",
    "\n",
    "# df_tfidf.reset_index(drop=True, inplace=True)\n",
    "# df_doc_vectors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# df_combined_vectors = pd.concat([df_tfidf, df_doc_vectors], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Linguistic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_features(text):\n",
    "#   doc = nlp(text)\n",
    "\n",
    "#   num_tokens = len(doc)\n",
    "#   num_nouns = sum(1 for token in doc if token.pos_ == 'NOUN')\n",
    "#   num_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "#   num_adjs = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
    "#   num_entities = len(doc.ents)\n",
    "#   num_person = sum(1 for ent in doc.ents if ent.label_ == 'PERSON')\n",
    "#   num_org = sum(1 for ent in doc.ents if ent.label_ == 'ORG')\n",
    "#   num_gpe = sum(1 for ent in doc.ents if ent.label_ == 'GPE')\n",
    "#   num_sentences = len(list(doc.sents))\n",
    "#   avg_sentence_length = np.mean([len(sent) for sent in doc.sents]) if num_sentences > 0 else 0\n",
    "#   num_urls = len(re.findall(r'http\\S+|www\\S+', text))\n",
    "\n",
    "#   return {\n",
    "#         \"num_tokens\": num_tokens,\n",
    "#         \"num_nouns\": num_nouns,\n",
    "#         \"num_verbs\": num_verbs,\n",
    "#         \"num_adjs\": num_adjs,\n",
    "#         \"num_entities\": num_entities,\n",
    "#         \"num_person\": num_person,\n",
    "#         \"num_org\": num_org,\n",
    "#         \"num_gpe\": num_gpe,\n",
    "#         \"num_sentences\": num_sentences,\n",
    "#         \"avg_sentence_length\": avg_sentence_length,\n",
    "#         \"num_urls\": num_urls\n",
    "#     }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_linguistic_features = pd.DataFrame(df_model['cleaned_text'].apply(extract_features).tolist())\n",
    "# df_linguistic_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pstat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
