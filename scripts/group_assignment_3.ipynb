{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade sympy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juSgjN339CmH",
        "outputId": "15671078-c1f2-4769-e4bd-e58eb1e3b273"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (1.13.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Binary class classification using pre-trained transformer Bert"
      ],
      "metadata": {
        "id": "tdumg1gKH4DX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Dps-wSai3mbu"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/pstat197a/claims_clean.csv\")\n",
        "\n",
        "label_encoder=LabelEncoder()\n",
        "df['bclass_encoded']=label_encoder.fit_transform(df['bclass'])\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels=train_test_split(\n",
        "    df['text_clean'].tolist(),\n",
        "    df['bclass_encoded'].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME=\"bert-base-uncased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "train_texts=[str(text) for text in train_texts]\n",
        "val_texts=[str(text) for text in val_texts]\n",
        "\n",
        "train_encodings=tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings=tokenizer(val_texts, truncation=True, padding=True, max_length=512)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J31cu_pP6sDQ",
        "outputId": "bfa218ad-8320-4043-a33b-b591c710c223"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ClaimsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings=encodings\n",
        "        self.labels=labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item={key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels']=torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset=ClaimsDataset(train_encodings, train_labels)\n",
        "val_dataset=ClaimsDataset(val_encodings, val_labels)\n"
      ],
      "metadata": {
        "id": "YHIETHj17QDN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "model=AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
        "\n",
        "training_args=TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels=pred.label_ids\n",
        "    preds=pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
        "    acc=accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "spgq-xLD7Z1A",
        "outputId": "8a1f55c2-9760-4aed-9815-888cc2c5d367"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-7-df1453bb2392>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer=Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='535' max='535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [535/535 14:29, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.531300</td>\n",
              "      <td>0.552246</td>\n",
              "      <td>0.724299</td>\n",
              "      <td>0.682796</td>\n",
              "      <td>0.881944</td>\n",
              "      <td>0.557018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.534100</td>\n",
              "      <td>0.451435</td>\n",
              "      <td>0.799065</td>\n",
              "      <td>0.823045</td>\n",
              "      <td>0.775194</td>\n",
              "      <td>0.877193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.353300</td>\n",
              "      <td>0.510872</td>\n",
              "      <td>0.813084</td>\n",
              "      <td>0.814815</td>\n",
              "      <td>0.862745</td>\n",
              "      <td>0.771930</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.325200</td>\n",
              "      <td>0.446610</td>\n",
              "      <td>0.824766</td>\n",
              "      <td>0.838013</td>\n",
              "      <td>0.825532</td>\n",
              "      <td>0.850877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.330300</td>\n",
              "      <td>0.495205</td>\n",
              "      <td>0.817757</td>\n",
              "      <td>0.828194</td>\n",
              "      <td>0.831858</td>\n",
              "      <td>0.824561</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=535, training_loss=0.4166504871065371, metrics={'train_runtime': 870.9614, 'train_samples_per_second': 9.828, 'train_steps_per_second': 0.614, 'total_flos': 2252230633881600.0, 'train_loss': 0.4166504871065371, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained(\"./binary_classification_model\")\n",
        "tokenizer.save_pretrained(\"./binary_classification_model\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "EQrvP5xR7dXn",
        "outputId": "d547779e-a1fd-47a4-ebf6-497fa1053f13"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 00:11]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.44660964608192444, 'eval_accuracy': 0.8247663551401869, 'eval_f1': 0.838012958963283, 'eval_precision': 0.825531914893617, 'eval_recall': 0.8508771929824561, 'eval_runtime': 11.7199, 'eval_samples_per_second': 36.519, 'eval_steps_per_second': 2.304, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./binary_classification_model/tokenizer_config.json',\n",
              " './binary_classification_model/special_tokens_map.json',\n",
              " './binary_classification_model/vocab.txt',\n",
              " './binary_classification_model/added_tokens.json',\n",
              " './binary_classification_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "test_encodings={key: val.to(device) for key, val in test_encodings.items()}\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs=model(**test_encodings)\n",
        "    predictions=torch.argmax(outputs.logits, dim=1)\n",
        "\n",
        "print(\"Predictions:\", predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NojDwMtN7fqf",
        "outputId": "95197b58-b0ba-4f1e-c545-5ed732711c17"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predictions: tensor([0, 0], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiclass classification"
      ],
      "metadata": {
        "id": "W30H1yoaIKqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/pstat197a/claims_clean.csv\")\n",
        "\n",
        "label_encoder=LabelEncoder()\n",
        "df[\"mclass_encoded\"]=label_encoder.fit_transform(df[\"mclass\"])\n",
        "\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    df[\"text_clean\"].tolist(),\n",
        "    df[\"mclass_encoded\"].tolist(),\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "nIbIbIMqIOHG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "MODEL_NAME=\"bert-base-uncased\"\n",
        "tokenizer=AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "train_texts=[str(text) for text in train_texts]\n",
        "val_texts=[str(text) for text in val_texts]\n",
        "train_encodings=tokenizer(train_texts, truncation=True, padding=True, max_length=512)\n",
        "val_encodings=tokenizer(val_texts, truncation=True, padding=True, max_length=512)"
      ],
      "metadata": {
        "id": "bsR9R7DEIo_X"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class ClaimsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings=encodings\n",
        "        self.labels=labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item={key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item[\"labels\"]=torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "train_dataset=ClaimsDataset(train_encodings, train_labels)\n",
        "val_dataset=ClaimsDataset(val_encodings, val_labels)"
      ],
      "metadata": {
        "id": "AfFKwcgII9yv"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "num_classes=len(label_encoder.classes_)\n",
        "model=AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_classes)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels=pred.label_ids\n",
        "    preds=pred.predictions.argmax(-1)\n",
        "    acc=accuracy_score(labels, preds)\n",
        "    f1=f1_score(labels, preds, average=\"weighted\")\n",
        "    precision=precision_score(labels, preds, average=\"weighted\")\n",
        "    recall=recall_score(labels, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "trainer=Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "A5sQkgn1JEyu",
        "outputId": "b805c11f-c364-43ee-ac6f-37b974e33b6d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-18-35c7c1e041ab>:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer=Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='535' max='535' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [535/535 17:07, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.605394</td>\n",
              "      <td>0.792056</td>\n",
              "      <td>0.771789</td>\n",
              "      <td>0.764250</td>\n",
              "      <td>0.792056</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.513613</td>\n",
              "      <td>0.827103</td>\n",
              "      <td>0.828077</td>\n",
              "      <td>0.830276</td>\n",
              "      <td>0.827103</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.481131</td>\n",
              "      <td>0.831776</td>\n",
              "      <td>0.830600</td>\n",
              "      <td>0.831909</td>\n",
              "      <td>0.831776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.457874</td>\n",
              "      <td>0.841121</td>\n",
              "      <td>0.840097</td>\n",
              "      <td>0.839908</td>\n",
              "      <td>0.841121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.572200</td>\n",
              "      <td>0.457002</td>\n",
              "      <td>0.845794</td>\n",
              "      <td>0.844719</td>\n",
              "      <td>0.845098</td>\n",
              "      <td>0.845794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=535, training_loss=0.5574191120183356, metrics={'train_runtime': 1029.1752, 'train_samples_per_second': 8.317, 'train_steps_per_second': 0.52, 'total_flos': 2252291299491840.0, 'train_loss': 0.5574191120183356, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results=trainer.evaluate()\n",
        "print(\"Evaluation Results:\", eval_results)\n",
        "\n",
        "model.save_pretrained(\"./multiclass_classification_model\")\n",
        "tokenizer.save_pretrained(\"./multiclass_classification_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "TzOSQ5GGN6L2",
        "outputId": "dc905de6-ed5f-45a3-e359-a106cebe8921"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [27/27 00:12]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Results: {'eval_loss': 0.4570024311542511, 'eval_accuracy': 0.8457943925233645, 'eval_f1': 0.8447187680126114, 'eval_precision': 0.8450982468398252, 'eval_recall': 0.8457943925233645, 'eval_runtime': 13.4727, 'eval_samples_per_second': 31.768, 'eval_steps_per_second': 2.004, 'epoch': 5.0}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./multiclass_classification_model/tokenizer_config.json',\n",
              " './multiclass_classification_model/special_tokens_map.json',\n",
              " './multiclass_classification_model/vocab.txt',\n",
              " './multiclass_classification_model/added_tokens.json',\n",
              " './multiclass_classification_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}