# -*- coding: utf-8 -*-
"""Deliverable 4 script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yWw6rS2NJ7apg3Em_lE4jFNqbGDgxf2J
"""

import pyreadr
import pandas as pd
from bs4 import BeautifulSoup
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

def load_test_data(test_data_path):
    test_data=pyreadr.read_r(test_data_path)
    test_df=test_data[list(test_data.keys())[0]]
    return test_df

def clean_html(raw_html):
    soup=BeautifulSoup(raw_html, "html.parser")
    text=soup.get_text(separator=" ")
    return text.strip()

def clean_test_data(test_df):
    test_df["text_clean"]=test_df["text_tmp"].apply(clean_html)
    return test_df

def tokenize_texts(texts, tokenizer, max_length=256):
    return tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors="pt")

def load_models(binary_model_path, multiclass_model_path):
    binary_model=AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=2)
    binary_model.load_state_dict(torch.load(binary_model_path))
    binary_model.eval()

    multiclass_model = AutoModelForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=5)  # Adjust num_labels as needed
    multiclass_model.load_state_dict(torch.load(multiclass_model_path))
    multiclass_model.eval()

    return binary_model, multiclass_model

def generate_predictions(test_encodings, binary_model, multiclass_model, device):
    test_encodings={key: val.to(device) for key, val in test_encodings.items()}

    with torch.no_grad():
        binary_outputs=binary_model(**test_encodings)
        binary_predictions=torch.argmax(binary_outputs.logits, dim=1)

    with torch.no_grad():
        multiclass_outputs=multiclass_model(**test_encodings)
        multiclass_predictions=torch.argmax(multiclass_outputs.logits, dim=1)

    return binary_predictions.cpu().numpy(), multiclass_predictions.cpu().numpy()

def save_predictions_to_rdata(test_df, binary_preds, multiclass_preds, output_path):
    from rpy2.robjects import pandas2ri
    import rpy2.robjects as ro

    test_df["bclass.pred"]=binary_preds
    test_df["mclass.pred"]=multiclass_preds
.
    pd.DataFrame.iteritems=pd.DataFrame.items
    pandas2ri.activate()
    r_test_df=pandas2ri.py2rpy(test_df)
    ro.r.assign("test_predictions", r_test_df)
    ro.r(f"save(test_predictions, file='{output_path}')")

def main():
    #test_data_path="/content/drive/MyDrive/pstat197a/claims-test.RData"
    #binary_model_path="/content/drive/MyDrive/pstat197a/binary_classification_model.pth"
    #multiclass_model_path="/content/drive/MyDrive/pstat197a/multiclass_classification_model.pth"
    #output_rdata_path="/content/drive/MyDrive/pstat197a/preds-groupN.RData"
    test_data_path = "../data/claims-test.RData"
    binary_model_path="../results/binary_classification_model.pth"
    multiclass_model_path="../results/multiclass_classification_model.pth"
    output_rdata_path = "../results/preds-groupN.RData"

    device=torch.device("cuda" if torch.cuda.is_available() else "cpu")

    test_df=load_test_data(test_data_path)
    test_df=clean_test_data(test_df)

    tokenizer=AutoTokenizer.from_pretrained("bert-base-uncased")
    test_texts=test_df["text_clean"].astype(str).tolist()
    test_encodings=tokenize_texts(test_texts, tokenizer)

    binary_model, multiclass_model=load_models(binary_model_path, multiclass_model_path)
    binary_model.to(device)
    multiclass_model.to(device)

    binary_preds, multiclass_preds=generate_predictions(test_encodings, binary_model, multiclass_model, device)

    save_predictions_to_rdata(test_df, binary_preds, multiclass_preds, output_rdata_path)
    print(f"Predictions saved to {output_rdata_path}")

if __name__=="__main__":
    main()