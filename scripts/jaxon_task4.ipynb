{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import regex\n",
    "from scripts.jaxon_scripts.preprocessing import parse_data, extract_paragraphs, extract_headers\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headers_paragraphs(df):\n",
    "    df[\"soup\"] = df[\"text_tmp\"].apply(lambda x: BeautifulSoup(x, \"html.parser\"))\n",
    "    df[\"headers\"] = df[\"soup\"].apply(extract_headers)\n",
    "    df[\"paragraphs\"] = df[\"soup\"].apply(extract_paragraphs)\n",
    "    df[\"text_clean\"] = df.apply(\n",
    "        lambda row: f\"{''.join(row['headers'])} \\n {''.join(row['paragraphs'])} \\n {row['text_clean']}\",\n",
    "        axis=1,\n",
    "    )\n",
    "    df = df.drop(columns=['headers', 'paragraphs', 'soup', 'Unnamed: 0'])\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower() # lower all the texts\n",
    "\n",
    "  # text = re.sub(r'http\\S+|www.\\S+', '', text) # remove all the links\n",
    "\n",
    "  doc = nlp(text)\n",
    "  cleaned_tokens = []\n",
    "\n",
    "  for token in doc:\n",
    "    if not token.is_punct and not token.is_stop: # remove punctuation and stop words\n",
    "      cleaned_tokens.append(token.lemma_) # lemmatization\n",
    "\n",
    "  return ' '.join(cleaned_tokens)\n",
    "\n",
    "def get_doc_vector(text):\n",
    "  doc = nlp(text)\n",
    "  return doc.vector\n",
    "\n",
    "def tfidf_unigram_vectorization(df):\n",
    "  df['doc_vector'] = df['text_clean'].apply(get_doc_vector) # use spacy to vectorize contents\n",
    "  df_doc_vectors = pd.DataFrame(df['doc_vector'].tolist())\n",
    "  df_doc_vectors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  tfidf = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "  tfidf_matrix = tfidf.fit_transform(df['text_clean']) # vectorize the texts\n",
    "  df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                          columns=tfidf.get_feature_names_out()) # construct a new df for features extracted using tfidf\n",
    "  df_tfidf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  df_unigram = pd.concat([df_tfidf, df_doc_vectors], axis=1) # combine spacy vectors and tfidf vectors\n",
    "\n",
    "  return df_unigram\n",
    "\n",
    "def tfidf_bigrams_vectorization(df):\n",
    "  # construct Tf Idf model with maximum 5000 features, capture only bigrams\n",
    "  tfidf = TfidfVectorizer(ngram_range=(2, 2), max_features=10000)\n",
    "  tfidf_matrix = tfidf.fit_transform(df['text_clean'])\n",
    "  df_bigram = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                          columns=tfidf.get_feature_names_out()) # construct a new df for features extracted using bigram\n",
    "  \n",
    "  return df_bigram\n",
    "\n",
    "def extract_features(text):\n",
    "  doc = nlp(text)\n",
    "\n",
    "  num_tokens = len(doc)\n",
    "  num_nouns = sum(1 for token in doc if token.pos_ == 'NOUN')\n",
    "  num_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "  num_adjs = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
    "  num_entities = len(doc.ents)\n",
    "  num_person = sum(1 for ent in doc.ents if ent.label_ == 'PERSON')\n",
    "  num_org = sum(1 for ent in doc.ents if ent.label_ == 'ORG')\n",
    "  num_gpe = sum(1 for ent in doc.ents if ent.label_ == 'GPE')\n",
    "  num_sentences = len(list(doc.sents))\n",
    "  avg_sentence_length = np.mean([len(sent) for sent in doc.sents]) if num_sentences > 0 else 0\n",
    "  num_urls = len(re.findall(r'http\\S+|www\\S+', text))\n",
    "\n",
    "  return {\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"num_nouns\": num_nouns,\n",
    "        \"num_verbs\": num_verbs,\n",
    "        \"num_adjs\": num_adjs,\n",
    "        \"num_entities\": num_entities,\n",
    "        \"num_person\": num_person,\n",
    "        \"num_org\": num_org,\n",
    "        \"num_gpe\": num_gpe,\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"num_urls\": num_urls\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Script\n",
    "df_raw = pd.read_csv('../data/claims_clean.csv') # read data\n",
    "\n",
    "# df_clean = parse_data(df_raw) # extract contents from html\n",
    "df_clean = extract_headers_paragraphs(df_raw) # extract headers and paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization & Lemmatization\n",
    "df_clean['text_clean'] = df_clean['text_clean'].apply(clean_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization\n",
    "df_unigrams = tfidf_unigram_vectorization(df_clean) # tfidf: Unigrams\n",
    "df_bigrams = tfidf_unigram_vectorization(df_clean) # tfidf: Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bclass = df_raw['bclass']\n",
    "y_mclass = df_raw['mclass']\n",
    "df_unigrams.columns = df_unigrams.columns.astype(str)\n",
    "df_bigrams.columns = df_bigrams.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_unigrams, X_test_unigrams, y_train_bclass, y_test_bclass = train_test_split(df_unigrams, y_bclass, stratify=y_bclass, test_size=0.3, random_state=197)\n",
    "\n",
    "### Logistic Principle Components Regression: Unigrams\n",
    "logreg_pca_clf = Pipeline([('scalar', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('logreg', LogisticRegression(solver='saga',\n",
    "                                                         penalty='l1',\n",
    "                                                         max_iter=5000,\n",
    "                                                         random_state=197))])\n",
    "\n",
    "logreg_grid_cv = GridSearchCV(logreg_pca_clf, \n",
    "                              {'pca__n_components': [50, 100, 200, 300, 500],\n",
    "                               'logreg__C': [0.01, 0.1, 1, 10, 100]}, \n",
    "                              cv=5, \n",
    "                              scoring='f1_weighted',\n",
    "                              n_jobs=-1)\n",
    "\n",
    "logreg_grid_cv.fit(X_train_unigrams, y_train_bclass)\n",
    "\n",
    "# get the log-odds of 174 features after pca\n",
    "logreg_unigrams_best = logreg_grid_cv.best_estimator_\n",
    "log_odds = logreg_unigrams_best.decision_function(df_unigrams)\n",
    "df_log_odds = pd.DataFrame(log_odds, columns=['log_odds'])\n",
    "\n",
    "# get pca of unigrams\n",
    "pca_unigram = PCA(logreg_grid_cv.best_params_['pca__n_components'])\n",
    "X_unigram_pca = pca_unigram.fit_transform(df_unigrams)\n",
    "df_pca_unigrams = pd.DataFrame(X_unigram_pca, columns=[f'pca_unigram_{i}' for i in range(X_unigram_pca.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pca_unigrams.to_csv('../data/X_pca_unigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_bigrams, X_test_bigrams, y_train_bclass, y_test_bclass = train_test_split(df_bigrams, y_bclass, stratify=y_bclass, test_size=0.3, random_state=197)\n",
    "\n",
    "### Logistic Principle Components Regression: Unigrams\n",
    "logreg_pca_clf = Pipeline([('scalar', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('logreg', LogisticRegression(solver='saga',\n",
    "                                                         penalty='l1',\n",
    "                                                         max_iter=5000,\n",
    "                                                         random_state=197))])\n",
    "\n",
    "logreg_grid_cv = GridSearchCV(logreg_pca_clf, \n",
    "                              {'pca__n_components': [50, 100, 200, 300, 500],\n",
    "                               'logreg__C': [0.01, 0.1, 1, 10, 100]}, \n",
    "                              cv=5, \n",
    "                              scoring='f1_weighted', \n",
    "                              n_jobs=-1)\n",
    "\n",
    "logreg_grid_cv.fit(X_train_bigrams, y_train_bclass)\n",
    "\n",
    "# get pca of bigrams\n",
    "pca_bigrams = PCA(logreg_grid_cv.best_params_['pca__n_components'])\n",
    "X_bigrams_pca = pca_bigrams.fit_transform(df_bigrams)\n",
    "df_pca_bigrams = pd.DataFrame(X_bigrams_pca, columns=[f'pca_bigrams_{i}' for i in range(X_bigrams_pca.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_POS = df_clean['text_clean'].apply(extract_features)\n",
    "\n",
    "### Concatenate all features\n",
    "df_features = pd.concat([df_POS, df_pca_unigrams, df_pca_bigrams, df_log_odds], axis=1)\n",
    "df_features = df_features.drop(columns=['text_clean'])\n",
    "\n",
    "X_train, X_test, y_train_bclass, y_test_bclass = train_test_split(df_features, y_bclass, stratify=y_bclass, test_size=0.3, random_state=197)\n",
    "\n",
    "### Logistic Principle Components Regression\n",
    "logreg_pca_clf = Pipeline([('scalar', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('logreg', LogisticRegression(solver='saga',\n",
    "                                                         penalty='l2',\n",
    "                                                         max_iter=5000,\n",
    "                                                         random_state=197))])\n",
    "\n",
    "logreg_grid_cv = GridSearchCV(logreg_pca_clf, \n",
    "                              {'pca__n_components': [50, 100, 150, 200, 250, 300, 400, 450, 500],\n",
    "                               'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]}, \n",
    "                              cv=5, \n",
    "                              scoring='f1_weighted',\n",
    "                              n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pca_unigram_0</th>\n",
       "      <th>pca_unigram_1</th>\n",
       "      <th>pca_unigram_2</th>\n",
       "      <th>pca_unigram_3</th>\n",
       "      <th>pca_unigram_4</th>\n",
       "      <th>pca_unigram_5</th>\n",
       "      <th>pca_unigram_6</th>\n",
       "      <th>pca_unigram_7</th>\n",
       "      <th>pca_unigram_8</th>\n",
       "      <th>pca_unigram_9</th>\n",
       "      <th>pca_unigram_10</th>\n",
       "      <th>pca_unigram_11</th>\n",
       "      <th>pca_unigram_12</th>\n",
       "      <th>pca_unigram_13</th>\n",
       "      <th>pca_unigram_14</th>\n",
       "      <th>pca_unigram_15</th>\n",
       "      <th>pca_unigram_16</th>\n",
       "      <th>pca_unigram_17</th>\n",
       "      <th>pca_unigram_18</th>\n",
       "      <th>pca_unigram_19</th>\n",
       "      <th>pca_unigram_20</th>\n",
       "      <th>pca_unigram_21</th>\n",
       "      <th>pca_unigram_22</th>\n",
       "      <th>pca_unigram_23</th>\n",
       "      <th>pca_unigram_24</th>\n",
       "      <th>pca_unigram_25</th>\n",
       "      <th>pca_unigram_26</th>\n",
       "      <th>pca_unigram_27</th>\n",
       "      <th>pca_unigram_28</th>\n",
       "      <th>pca_unigram_29</th>\n",
       "      <th>pca_unigram_30</th>\n",
       "      <th>pca_unigram_31</th>\n",
       "      <th>pca_unigram_32</th>\n",
       "      <th>pca_unigram_33</th>\n",
       "      <th>pca_unigram_34</th>\n",
       "      <th>pca_unigram_35</th>\n",
       "      <th>pca_unigram_36</th>\n",
       "      <th>pca_unigram_37</th>\n",
       "      <th>pca_unigram_38</th>\n",
       "      <th>pca_unigram_39</th>\n",
       "      <th>...</th>\n",
       "      <th>pca_bigrams_461</th>\n",
       "      <th>pca_bigrams_462</th>\n",
       "      <th>pca_bigrams_463</th>\n",
       "      <th>pca_bigrams_464</th>\n",
       "      <th>pca_bigrams_465</th>\n",
       "      <th>pca_bigrams_466</th>\n",
       "      <th>pca_bigrams_467</th>\n",
       "      <th>pca_bigrams_468</th>\n",
       "      <th>pca_bigrams_469</th>\n",
       "      <th>pca_bigrams_470</th>\n",
       "      <th>pca_bigrams_471</th>\n",
       "      <th>pca_bigrams_472</th>\n",
       "      <th>pca_bigrams_473</th>\n",
       "      <th>pca_bigrams_474</th>\n",
       "      <th>pca_bigrams_475</th>\n",
       "      <th>pca_bigrams_476</th>\n",
       "      <th>pca_bigrams_477</th>\n",
       "      <th>pca_bigrams_478</th>\n",
       "      <th>pca_bigrams_479</th>\n",
       "      <th>pca_bigrams_480</th>\n",
       "      <th>pca_bigrams_481</th>\n",
       "      <th>pca_bigrams_482</th>\n",
       "      <th>pca_bigrams_483</th>\n",
       "      <th>pca_bigrams_484</th>\n",
       "      <th>pca_bigrams_485</th>\n",
       "      <th>pca_bigrams_486</th>\n",
       "      <th>pca_bigrams_487</th>\n",
       "      <th>pca_bigrams_488</th>\n",
       "      <th>pca_bigrams_489</th>\n",
       "      <th>pca_bigrams_490</th>\n",
       "      <th>pca_bigrams_491</th>\n",
       "      <th>pca_bigrams_492</th>\n",
       "      <th>pca_bigrams_493</th>\n",
       "      <th>pca_bigrams_494</th>\n",
       "      <th>pca_bigrams_495</th>\n",
       "      <th>pca_bigrams_496</th>\n",
       "      <th>pca_bigrams_497</th>\n",
       "      <th>pca_bigrams_498</th>\n",
       "      <th>pca_bigrams_499</th>\n",
       "      <th>log_odds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>0.143770</td>\n",
       "      <td>0.021761</td>\n",
       "      <td>0.334757</td>\n",
       "      <td>-0.052748</td>\n",
       "      <td>-0.359876</td>\n",
       "      <td>-0.059881</td>\n",
       "      <td>-0.086740</td>\n",
       "      <td>-0.037649</td>\n",
       "      <td>-0.076151</td>\n",
       "      <td>-0.208160</td>\n",
       "      <td>-0.031777</td>\n",
       "      <td>-0.068252</td>\n",
       "      <td>-0.101144</td>\n",
       "      <td>-0.042978</td>\n",
       "      <td>-0.039656</td>\n",
       "      <td>0.058458</td>\n",
       "      <td>0.085597</td>\n",
       "      <td>-0.002647</td>\n",
       "      <td>0.032444</td>\n",
       "      <td>-0.014030</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.183291</td>\n",
       "      <td>-0.228761</td>\n",
       "      <td>-0.048227</td>\n",
       "      <td>0.061764</td>\n",
       "      <td>-0.098722</td>\n",
       "      <td>-0.035638</td>\n",
       "      <td>0.043198</td>\n",
       "      <td>-0.009750</td>\n",
       "      <td>0.003044</td>\n",
       "      <td>-0.064662</td>\n",
       "      <td>0.022681</td>\n",
       "      <td>0.028559</td>\n",
       "      <td>0.020491</td>\n",
       "      <td>-0.016353</td>\n",
       "      <td>-0.098980</td>\n",
       "      <td>-0.001438</td>\n",
       "      <td>-0.025997</td>\n",
       "      <td>0.027246</td>\n",
       "      <td>-0.008381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.048137</td>\n",
       "      <td>0.022260</td>\n",
       "      <td>0.013549</td>\n",
       "      <td>-0.021695</td>\n",
       "      <td>-0.008624</td>\n",
       "      <td>-0.010209</td>\n",
       "      <td>0.009333</td>\n",
       "      <td>0.056848</td>\n",
       "      <td>0.048360</td>\n",
       "      <td>0.037704</td>\n",
       "      <td>0.007790</td>\n",
       "      <td>-0.018789</td>\n",
       "      <td>-0.056626</td>\n",
       "      <td>-0.007340</td>\n",
       "      <td>0.029107</td>\n",
       "      <td>-0.030569</td>\n",
       "      <td>0.012756</td>\n",
       "      <td>0.041291</td>\n",
       "      <td>0.019403</td>\n",
       "      <td>-0.046639</td>\n",
       "      <td>0.043230</td>\n",
       "      <td>-0.007550</td>\n",
       "      <td>0.006101</td>\n",
       "      <td>0.004048</td>\n",
       "      <td>-0.021570</td>\n",
       "      <td>-0.053426</td>\n",
       "      <td>-0.045199</td>\n",
       "      <td>0.003957</td>\n",
       "      <td>0.048429</td>\n",
       "      <td>-0.004697</td>\n",
       "      <td>-0.031505</td>\n",
       "      <td>0.079777</td>\n",
       "      <td>-0.048608</td>\n",
       "      <td>-0.024121</td>\n",
       "      <td>0.004775</td>\n",
       "      <td>-0.004540</td>\n",
       "      <td>-0.055017</td>\n",
       "      <td>0.068907</td>\n",
       "      <td>-0.038325</td>\n",
       "      <td>1.122890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>810</th>\n",
       "      <td>1.014193</td>\n",
       "      <td>0.650175</td>\n",
       "      <td>-0.810739</td>\n",
       "      <td>-0.432737</td>\n",
       "      <td>0.510579</td>\n",
       "      <td>0.257711</td>\n",
       "      <td>0.419720</td>\n",
       "      <td>-0.704564</td>\n",
       "      <td>-0.283096</td>\n",
       "      <td>-0.424603</td>\n",
       "      <td>-0.423608</td>\n",
       "      <td>0.299818</td>\n",
       "      <td>-0.250524</td>\n",
       "      <td>-0.084016</td>\n",
       "      <td>-0.092922</td>\n",
       "      <td>-0.066170</td>\n",
       "      <td>-0.029683</td>\n",
       "      <td>-0.003192</td>\n",
       "      <td>0.130163</td>\n",
       "      <td>-0.009336</td>\n",
       "      <td>0.034981</td>\n",
       "      <td>0.074429</td>\n",
       "      <td>-0.023362</td>\n",
       "      <td>-0.056013</td>\n",
       "      <td>-0.033992</td>\n",
       "      <td>-0.030065</td>\n",
       "      <td>0.058296</td>\n",
       "      <td>-0.103729</td>\n",
       "      <td>0.097854</td>\n",
       "      <td>-0.168880</td>\n",
       "      <td>0.052644</td>\n",
       "      <td>-0.037727</td>\n",
       "      <td>0.075840</td>\n",
       "      <td>-0.025230</td>\n",
       "      <td>0.132827</td>\n",
       "      <td>0.000537</td>\n",
       "      <td>0.005240</td>\n",
       "      <td>-0.042019</td>\n",
       "      <td>-0.086120</td>\n",
       "      <td>-0.044886</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005956</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>-0.001294</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>-0.001874</td>\n",
       "      <td>-0.008534</td>\n",
       "      <td>0.011205</td>\n",
       "      <td>-0.012575</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>-0.000679</td>\n",
       "      <td>0.004524</td>\n",
       "      <td>-0.007291</td>\n",
       "      <td>0.006446</td>\n",
       "      <td>-0.016154</td>\n",
       "      <td>-0.008952</td>\n",
       "      <td>0.001529</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>-0.012551</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>-0.004960</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>-0.007528</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.008054</td>\n",
       "      <td>-0.008248</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.008297</td>\n",
       "      <td>-0.009010</td>\n",
       "      <td>-0.007588</td>\n",
       "      <td>-0.005120</td>\n",
       "      <td>-0.009848</td>\n",
       "      <td>-0.003437</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.005574</td>\n",
       "      <td>0.875100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0.438736</td>\n",
       "      <td>-0.364980</td>\n",
       "      <td>-1.450496</td>\n",
       "      <td>-0.742925</td>\n",
       "      <td>-0.985758</td>\n",
       "      <td>-0.219200</td>\n",
       "      <td>0.407037</td>\n",
       "      <td>-0.080868</td>\n",
       "      <td>-0.135363</td>\n",
       "      <td>0.862196</td>\n",
       "      <td>0.368198</td>\n",
       "      <td>0.304471</td>\n",
       "      <td>-0.029362</td>\n",
       "      <td>-0.222132</td>\n",
       "      <td>-0.025991</td>\n",
       "      <td>0.009432</td>\n",
       "      <td>0.010211</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.079738</td>\n",
       "      <td>-0.035120</td>\n",
       "      <td>0.002059</td>\n",
       "      <td>0.135361</td>\n",
       "      <td>-0.022616</td>\n",
       "      <td>-0.026983</td>\n",
       "      <td>-0.013801</td>\n",
       "      <td>0.064022</td>\n",
       "      <td>0.066022</td>\n",
       "      <td>-0.040885</td>\n",
       "      <td>-0.036912</td>\n",
       "      <td>-0.041513</td>\n",
       "      <td>-0.029415</td>\n",
       "      <td>-0.003515</td>\n",
       "      <td>-0.029061</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>-0.032610</td>\n",
       "      <td>-0.016834</td>\n",
       "      <td>0.030944</td>\n",
       "      <td>0.045358</td>\n",
       "      <td>-0.005487</td>\n",
       "      <td>-0.043480</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004809</td>\n",
       "      <td>0.005374</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>-0.003281</td>\n",
       "      <td>-0.000084</td>\n",
       "      <td>-0.004203</td>\n",
       "      <td>0.006696</td>\n",
       "      <td>-0.004691</td>\n",
       "      <td>0.003353</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.005142</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>0.005228</td>\n",
       "      <td>0.002353</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>-0.000635</td>\n",
       "      <td>-0.001974</td>\n",
       "      <td>-0.001172</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>0.003061</td>\n",
       "      <td>-0.001298</td>\n",
       "      <td>0.002687</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.001480</td>\n",
       "      <td>0.004393</td>\n",
       "      <td>-0.002702</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.001171</td>\n",
       "      <td>-0.005278</td>\n",
       "      <td>-0.001968</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>1.368829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1857</th>\n",
       "      <td>0.158307</td>\n",
       "      <td>-0.109818</td>\n",
       "      <td>-0.580149</td>\n",
       "      <td>-0.119075</td>\n",
       "      <td>0.036250</td>\n",
       "      <td>0.067032</td>\n",
       "      <td>-0.032986</td>\n",
       "      <td>-0.157882</td>\n",
       "      <td>-0.014882</td>\n",
       "      <td>-0.339794</td>\n",
       "      <td>0.121000</td>\n",
       "      <td>-0.086459</td>\n",
       "      <td>0.217297</td>\n",
       "      <td>-0.069518</td>\n",
       "      <td>0.096395</td>\n",
       "      <td>0.198042</td>\n",
       "      <td>0.142816</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>-0.004595</td>\n",
       "      <td>0.186183</td>\n",
       "      <td>-0.009914</td>\n",
       "      <td>0.025301</td>\n",
       "      <td>0.124835</td>\n",
       "      <td>-0.051517</td>\n",
       "      <td>-0.016324</td>\n",
       "      <td>0.046386</td>\n",
       "      <td>-0.121323</td>\n",
       "      <td>-0.102268</td>\n",
       "      <td>-0.045364</td>\n",
       "      <td>0.095417</td>\n",
       "      <td>-0.123275</td>\n",
       "      <td>-0.122605</td>\n",
       "      <td>-0.142969</td>\n",
       "      <td>0.054802</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.134133</td>\n",
       "      <td>0.051421</td>\n",
       "      <td>-0.025244</td>\n",
       "      <td>-0.062142</td>\n",
       "      <td>0.037074</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016848</td>\n",
       "      <td>0.009103</td>\n",
       "      <td>0.061097</td>\n",
       "      <td>0.021255</td>\n",
       "      <td>-0.000286</td>\n",
       "      <td>-0.011258</td>\n",
       "      <td>0.035603</td>\n",
       "      <td>0.036740</td>\n",
       "      <td>-0.022521</td>\n",
       "      <td>-0.002727</td>\n",
       "      <td>-0.005435</td>\n",
       "      <td>-0.012802</td>\n",
       "      <td>-0.017915</td>\n",
       "      <td>0.036843</td>\n",
       "      <td>0.034950</td>\n",
       "      <td>-0.040837</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>-0.028165</td>\n",
       "      <td>0.028161</td>\n",
       "      <td>-0.006323</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>-0.008370</td>\n",
       "      <td>-0.025598</td>\n",
       "      <td>-0.028656</td>\n",
       "      <td>0.003266</td>\n",
       "      <td>0.043498</td>\n",
       "      <td>0.010977</td>\n",
       "      <td>0.039331</td>\n",
       "      <td>-0.062793</td>\n",
       "      <td>0.010537</td>\n",
       "      <td>-0.054107</td>\n",
       "      <td>0.008672</td>\n",
       "      <td>-0.032118</td>\n",
       "      <td>0.021021</td>\n",
       "      <td>0.022251</td>\n",
       "      <td>-0.031710</td>\n",
       "      <td>0.011282</td>\n",
       "      <td>0.018712</td>\n",
       "      <td>-3.571587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>-0.718041</td>\n",
       "      <td>-0.536753</td>\n",
       "      <td>0.145944</td>\n",
       "      <td>-0.419451</td>\n",
       "      <td>0.139133</td>\n",
       "      <td>-0.012717</td>\n",
       "      <td>-0.102847</td>\n",
       "      <td>0.039802</td>\n",
       "      <td>0.011742</td>\n",
       "      <td>-0.038327</td>\n",
       "      <td>-0.061968</td>\n",
       "      <td>0.078876</td>\n",
       "      <td>-0.001293</td>\n",
       "      <td>0.011729</td>\n",
       "      <td>-0.014526</td>\n",
       "      <td>-0.004415</td>\n",
       "      <td>0.109976</td>\n",
       "      <td>0.005098</td>\n",
       "      <td>0.070303</td>\n",
       "      <td>-0.077762</td>\n",
       "      <td>-0.017733</td>\n",
       "      <td>0.046149</td>\n",
       "      <td>0.035013</td>\n",
       "      <td>0.051368</td>\n",
       "      <td>-0.078824</td>\n",
       "      <td>-0.070865</td>\n",
       "      <td>-0.048537</td>\n",
       "      <td>0.051728</td>\n",
       "      <td>-0.071740</td>\n",
       "      <td>-0.163874</td>\n",
       "      <td>0.034691</td>\n",
       "      <td>-0.118822</td>\n",
       "      <td>-0.039557</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>-0.000787</td>\n",
       "      <td>0.012097</td>\n",
       "      <td>0.055169</td>\n",
       "      <td>-0.066366</td>\n",
       "      <td>0.038315</td>\n",
       "      <td>-0.017981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024630</td>\n",
       "      <td>0.013930</td>\n",
       "      <td>-0.020626</td>\n",
       "      <td>-0.010655</td>\n",
       "      <td>0.015173</td>\n",
       "      <td>0.033630</td>\n",
       "      <td>-0.026449</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>0.077801</td>\n",
       "      <td>-0.023208</td>\n",
       "      <td>0.028772</td>\n",
       "      <td>-0.006143</td>\n",
       "      <td>-0.052434</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>-0.000798</td>\n",
       "      <td>-0.006194</td>\n",
       "      <td>0.032569</td>\n",
       "      <td>0.015205</td>\n",
       "      <td>-0.005064</td>\n",
       "      <td>0.038318</td>\n",
       "      <td>0.031505</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>-0.006240</td>\n",
       "      <td>-0.043739</td>\n",
       "      <td>0.028796</td>\n",
       "      <td>-0.022969</td>\n",
       "      <td>-0.014047</td>\n",
       "      <td>0.019583</td>\n",
       "      <td>-0.011141</td>\n",
       "      <td>0.042284</td>\n",
       "      <td>0.008368</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>0.005147</td>\n",
       "      <td>-0.031691</td>\n",
       "      <td>-0.001058</td>\n",
       "      <td>-0.012301</td>\n",
       "      <td>2.244645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 701 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pca_unigram_0  pca_unigram_1  ...  pca_bigrams_499  log_odds\n",
       "1399       0.143770       0.021761  ...        -0.038325  1.122890\n",
       "810        1.014193       0.650175  ...         0.005574  0.875100\n",
       "380        0.438736      -0.364980  ...         0.001164  1.368829\n",
       "1857       0.158307      -0.109818  ...         0.018712 -3.571587\n",
       "2030      -0.718041      -0.536753  ...        -0.012301  2.244645\n",
       "\n",
       "[5 rows x 701 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()),\n",
       "                                       (&#x27;pca&#x27;, PCA()),\n",
       "                                       (&#x27;logreg&#x27;,\n",
       "                                        LogisticRegression(max_iter=5000,\n",
       "                                                           random_state=197,\n",
       "                                                           solver=&#x27;saga&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;logreg__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;pca__n_components&#x27;: [50, 100, 150, 200, 250, 300, 400,\n",
       "                                               450, 500]},\n",
       "             scoring=&#x27;f1_weighted&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;GridSearchCV<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html\">?<span>Documentation for GridSearchCV</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()),\n",
       "                                       (&#x27;pca&#x27;, PCA()),\n",
       "                                       (&#x27;logreg&#x27;,\n",
       "                                        LogisticRegression(max_iter=5000,\n",
       "                                                           random_state=197,\n",
       "                                                           solver=&#x27;saga&#x27;))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;logreg__C&#x27;: [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         &#x27;pca__n_components&#x27;: [50, 100, 150, 200, 250, 300, 400,\n",
       "                                               450, 500]},\n",
       "             scoring=&#x27;f1_weighted&#x27;)</pre></div> </div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">best_estimator_: Pipeline</label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;scalar&#x27;, StandardScaler()), (&#x27;pca&#x27;, PCA(n_components=450)),\n",
       "                (&#x27;logreg&#x27;,\n",
       "                 LogisticRegression(C=0.001, max_iter=5000, random_state=197,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;StandardScaler<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.StandardScaler.html\">?<span>Documentation for StandardScaler</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>StandardScaler()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;PCA<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.decomposition.PCA.html\">?<span>Documentation for PCA</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>PCA(n_components=450)</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(C=0.001, max_iter=5000, random_state=197, solver=&#x27;saga&#x27;)</pre></div> </div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('scalar', StandardScaler()),\n",
       "                                       ('pca', PCA()),\n",
       "                                       ('logreg',\n",
       "                                        LogisticRegression(max_iter=5000,\n",
       "                                                           random_state=197,\n",
       "                                                           solver='saga'))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
       "                         'pca__n_components': [50, 100, 150, 200, 250, 300, 400,\n",
       "                                               450, 500]},\n",
       "             scoring='f1_weighted')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_grid_cv.fit(X_train, y_train_bclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Training F1 Score is 0.8068913450949079\n"
     ]
    }
   ],
   "source": [
    "best_logreg_bclass = logreg_grid_cv.best_estimator_\n",
    "print(f\"Weighted Training F1 Score is {logreg_grid_cv.best_score_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_bclass_pred = best_logreg_bclass.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weighted Testing F1 Score is 0.8074038144744611\n"
     ]
    }
   ],
   "source": [
    "print(f\"Weighted Testing F1 Score is {f1_score(y_test_bclass, y_bclass_pred, average='weighted')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pstat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
