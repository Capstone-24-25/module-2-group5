{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/spacy/util.py:910: UserWarning: [W095] Model 'en_core_web_lg' (3.8.0) was trained with spaCy v3.8.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import regex\n",
    "from preprocessing import parse_data, extract_paragraphs, extract_headers\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import roc_auc_score, classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 1500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_headers_paragraphs(df):\n",
    "    df[\"soup\"] = df[\"text_tmp\"].apply(lambda x: BeautifulSoup(x, \"html.parser\"))\n",
    "    df[\"headers\"] = df[\"soup\"].apply(extract_headers)\n",
    "    df[\"paragraphs\"] = df[\"soup\"].apply(extract_paragraphs)\n",
    "    df[\"text_clean\"] = df.apply(\n",
    "        lambda row: f\"{''.join(row['headers'])} \\n {''.join(row['paragraphs'])} \\n {row['text_clean']}\",\n",
    "        axis=1,\n",
    "    )\n",
    "    df = df.drop(columns=['headers', 'paragraphs', 'soup', 'Unnamed: 0'])\n",
    "    return df\n",
    "\n",
    "def clean_text(text):\n",
    "  text = text.lower() # lower all the texts\n",
    "\n",
    "  # text = re.sub(r'http\\S+|www.\\S+', '', text) # remove all the links\n",
    "\n",
    "  doc = nlp(text)\n",
    "  cleaned_tokens = []\n",
    "\n",
    "  for token in doc:\n",
    "    if not token.is_punct and not token.is_stop: # remove punctuation and stop words\n",
    "      cleaned_tokens.append(token.lemma_) # lemmatization\n",
    "\n",
    "  return ' '.join(cleaned_tokens)\n",
    "\n",
    "def get_doc_vector(text):\n",
    "  doc = nlp(text)\n",
    "  return doc.vector\n",
    "\n",
    "def tfidf_unigram_vectorization(df):\n",
    "  df['doc_vector'] = df['text_clean'].apply(get_doc_vector) # use spacy to vectorize contents\n",
    "  df_doc_vectors = pd.DataFrame(df['doc_vector'].tolist())\n",
    "  df_doc_vectors.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  tfidf = TfidfVectorizer(ngram_range=(1, 1), max_features=10000)\n",
    "  tfidf_matrix = tfidf.fit_transform(df['text_clean']) # vectorize the texts\n",
    "  df_tfidf = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                          columns=tfidf.get_feature_names_out()) # construct a new df for features extracted using tfidf\n",
    "  df_tfidf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  df_unigram = pd.concat([df_tfidf, df_doc_vectors], axis=1) # combine spacy vectors and tfidf vectors\n",
    "\n",
    "  return df_unigram\n",
    "\n",
    "def tfidf_bigrams_vectorization(df):\n",
    "  # construct Tf Idf model with maximum 5000 features, capture only bigrams\n",
    "  tfidf = TfidfVectorizer(ngram_range=(2, 2), max_features=10000)\n",
    "  tfidf_matrix = tfidf.fit_transform(df['text_clean'])\n",
    "  df_bigram = pd.DataFrame(tfidf_matrix.toarray(),\n",
    "                          columns=tfidf.get_feature_names_out()) # construct a new df for features extracted using bigram\n",
    "  \n",
    "  return df_bigram\n",
    "\n",
    "def extract_features(text):\n",
    "  doc = nlp(text)\n",
    "\n",
    "  num_tokens = len(doc)\n",
    "  num_nouns = sum(1 for token in doc if token.pos_ == 'NOUN')\n",
    "  num_verbs = sum(1 for token in doc if token.pos_ == 'VERB')\n",
    "  num_adjs = sum(1 for token in doc if token.pos_ == 'ADJ')\n",
    "  num_entities = len(doc.ents)\n",
    "  num_person = sum(1 for ent in doc.ents if ent.label_ == 'PERSON')\n",
    "  num_org = sum(1 for ent in doc.ents if ent.label_ == 'ORG')\n",
    "  num_gpe = sum(1 for ent in doc.ents if ent.label_ == 'GPE')\n",
    "  num_sentences = len(list(doc.sents))\n",
    "  avg_sentence_length = np.mean([len(sent) for sent in doc.sents]) if num_sentences > 0 else 0\n",
    "  num_urls = len(re.findall(r'http\\S+|www\\S+', text))\n",
    "\n",
    "  return {\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"num_nouns\": num_nouns,\n",
    "        \"num_verbs\": num_verbs,\n",
    "        \"num_adjs\": num_adjs,\n",
    "        \"num_entities\": num_entities,\n",
    "        \"num_person\": num_person,\n",
    "        \"num_org\": num_org,\n",
    "        \"num_gpe\": num_gpe,\n",
    "        \"num_sentences\": num_sentences,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"num_urls\": num_urls\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preprocessing Script\n",
    "df_raw = pd.read_csv('../data/claims_clean.csv') # read data\n",
    "\n",
    "# df_clean = parse_data(df_raw) # extract contents from html\n",
    "df_clean = extract_headers_paragraphs(df_raw) # extract headers and paragraphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Tokenization & Lemmatization\n",
    "df_clean['text_clean'] = df_clean['text_clean'].apply(clean_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Vectorization\n",
    "df_unigrams = tfidf_unigram_vectorization(df_clean) # tfidf: Unigrams\n",
    "df_bigrams = tfidf_unigram_vectorization(df_clean) # tfidf: Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bclass = df_raw['bclass']\n",
    "y_mclass = df_raw['mclass']\n",
    "df_unigrams.columns = df_unigrams.columns.astype(str)\n",
    "df_bigrams.columns = df_bigrams.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_unigrams, X_test_unigrams, y_train_bclass, y_test_bclass = train_test_split(df_unigrams, y_bclass, stratify=y_bclass, test_size=0.3, random_state=197)\n",
    "\n",
    "### Logistic Principle Components Regression: Unigrams\n",
    "logreg_pca_clf = Pipeline([('scalar', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('logreg', LogisticRegression(solver='saga',\n",
    "                                                         penalty='l2',\n",
    "                                                         max_iter=5000,\n",
    "                                                         random_state=197))])\n",
    "\n",
    "logreg_grid_cv = GridSearchCV(logreg_pca_clf, \n",
    "                              {'pca__n_components': [50, 100, 200, 300, 500],\n",
    "                               'logreg__C': [0.01, 0.1, 1, 10, 100]}, \n",
    "                              cv=5, \n",
    "                              scoring='f1_weighted',\n",
    "                              n_jobs=-1)\n",
    "\n",
    "logreg_grid_cv.fit(X_train_unigrams, y_train_bclass)\n",
    "\n",
    "# get the log-odds of 174 features after pca\n",
    "logreg_unigrams_best = logreg_grid_cv.best_estimator_\n",
    "log_odds = logreg_unigrams_best.decision_function(df_unigrams)\n",
    "df_log_odds = pd.DataFrame(log_odds, columns=['log_odds'])\n",
    "\n",
    "# get pca of unigrams\n",
    "pca_unigram = PCA(logreg_grid_cv.best_params_['pca__n_components'])\n",
    "X_unigram_pca = pca_unigram.fit_transform(df_unigrams)\n",
    "df_pca_unigrams = pd.DataFrame(X_unigram_pca, columns=[f'pca_unigram_{i}' for i in range(X_unigram_pca.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_grid_cv.best_params_['pca__n_components']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca_unigrams.to_csv('../data/X_pca_unigrams.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/pstat/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_bigrams, X_test_bigrams, y_train_bclass, y_test_bclass = train_test_split(df_bigrams, y_bclass, stratify=y_bclass, test_size=0.3, random_state=197)\n",
    "\n",
    "### Logistic Principle Components Regression: Unigrams\n",
    "logreg_pca_clf = Pipeline([('scalar', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('logreg', LogisticRegression(solver='saga',\n",
    "                                                         penalty='l2',\n",
    "                                                         max_iter=5000,\n",
    "                                                         random_state=197))])\n",
    "\n",
    "logreg_grid_cv = GridSearchCV(logreg_pca_clf, \n",
    "                              {'pca__n_components': [50, 100, 200, 300, 500],\n",
    "                               'logreg__C': [0.01, 0.1, 1, 10, 100]}, \n",
    "                              cv=5, \n",
    "                              scoring='f1_weighted', \n",
    "                              n_jobs=-1)\n",
    "\n",
    "logreg_grid_cv.fit(X_train_bigrams, y_train_bclass)\n",
    "\n",
    "# get pca of bigrams\n",
    "pca_bigrams = PCA(logreg_grid_cv.best_params_['pca__n_components'])\n",
    "X_bigrams_pca = pca_bigrams.fit_transform(df_bigrams)\n",
    "df_pca_bigrams = pd.DataFrame(X_bigrams_pca, columns=[f'pca_bigrams_{i}' for i in range(X_bigrams_pca.shape[1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_POS = df_clean['text_clean'].apply(extract_features)\n",
    "\n",
    "### Concatenate all features\n",
    "df_features = pd.concat([df_POS, df_pca_unigrams, df_pca_bigrams, df_log_odds], axis=1)\n",
    "\n",
    "X_train, X_test, y_train_bclass, y_test_bclass = train_test_split(df_features, y_bclass, stratify=y_bclass, test_size=0.3, random_state=197)\n",
    "\n",
    "### Logistic Principle Components Regression\n",
    "logreg_pca_clf = Pipeline([('scalar', StandardScaler()),\n",
    "                           ('pca', PCA()),\n",
    "                           ('logreg', LogisticRegression(solver='saga',\n",
    "                                                         penalty='l2',\n",
    "                                                         max_iter=5000,\n",
    "                                                         random_state=197))])\n",
    "\n",
    "logreg_grid_cv = GridSearchCV(logreg_pca_clf, \n",
    "                              {'pca__n_components': [50, 100, 150, 200, 250, 300, 400, 450, 500],\n",
    "                               'logreg__C': [0.001, 0.01, 0.1, 1, 10, 100]}, \n",
    "                              cv=5, \n",
    "                              scoring='f1_weighted',\n",
    "                              n_jobs=-1)\n",
    "\n",
    "logreg_grid_cv.fit(X_train, y_train_bclass)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_logreg_bclass = logreg_grid_cv.best_estimator_\n",
    "y_bclass_pred = best_logreg_bclass.predict(X_test)\n",
    "print(f\"Weighted F1 Score is {f1_score(y_bclass, y_bclass_pred, average='weighted')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pstat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
